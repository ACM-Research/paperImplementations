{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='This is a demonstration program')\n",
    "# # parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batchsize')\n",
    "# args = parser.parse_args()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 10000\n",
    "lr = 1e-4\n",
    "eval_iters = 500\n",
    "eval_interval = 500\n",
    "n_embed = 384 # number of elements in the embedding vector\n",
    "n_head = 16\n",
    "n_layer = 16 #num of decoder layers\n",
    "dropout = 0.2 # Dropout probability (makes model generalize more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32171"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "\n",
    "\n",
    "vocab_size = len(chars)\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dictionary mapping strings to int and int to string\n",
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
    "# Turns text into int\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "# Turns into into text\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else 'val_split.txt'\n",
    "    # Open file in binary\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Open with memory mapping\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            # Making end size before actual end so no memory out of bounds errors\n",
    "            start_pos = random.randint(0,(file_size) - block_size*batch_size)\n",
    "    \n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            # Decode\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model:nn.Module):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model.forward(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return  out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a batch of text sequences represented as a tensor with shape \\( B \\times T \\times C \\):\n",
    "\n",
    "- **Batch Size (B)**: If \\( $B = 64$ \\), there are 64 sequences in the batch.\n",
    "- **Sequence Length (T)**: If \\( $T = 128$ \\), each sequence has 128 tokens.\n",
    "- **Channel Dimension (C)**: If \\( $C = 512$ \\), each token is represented by a 512-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''one head of self-attention'''\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Wk, Wq, Wv parameters\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -.5 # That end bit is just sqrt d_k\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] ==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # For every head, forward on each of them and cat them\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(    nn.Linear(n_embed, 4 * n_embed), nn.ReLU(), nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # Decoder Block\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head #num of features each head captures\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # Self Attention\n",
    "        self.ffwd = FeedForward(n_embed) # Feed Forward Linear -> ReLU -> Linear\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # LayerNorm\n",
    "        self.ln2 = nn.LayerNorm(n_embed) # LayerNorm\n",
    "    def forward(self,x):\n",
    "        y = self.sa(x) # Y is self attention and x is added later\n",
    "        x = self.ln1(x+y) # Residual connection for self attention layer\n",
    "        \n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y) # resiudual connection for feed forwards\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3745, -0.6510, -0.7120,  ..., -0.5765,  1.7241,  2.0743],\n",
       "         [ 1.2201, -0.7214, -1.0183,  ..., -1.8096,  0.7192,  1.1727],\n",
       "         [ 1.1013, -1.2056, -0.1245,  ..., -0.9044, -0.5779,  0.9289],\n",
       "         ...,\n",
       "         [ 0.8428,  0.2611, -0.5450,  ..., -0.3930, -0.8016, -0.0912],\n",
       "         [ 1.8818,  0.6479,  0.2682,  ..., -2.1176,  0.6618,  0.2705],\n",
       "         [ 0.8391,  0.4207, -1.9776,  ...,  0.6284, -1.1361, -0.2795]],\n",
       "\n",
       "        [[ 1.2216, -1.8785,  0.1243,  ..., -0.0300, -0.9993,  0.5460],\n",
       "         [ 1.4146, -0.1245, -1.1276,  ...,  0.1211, -0.6716, -1.5616],\n",
       "         [ 0.5980, -1.5023,  0.5972,  ...,  0.5012, -1.1951,  1.2981],\n",
       "         ...,\n",
       "         [-0.3415, -0.3822, -1.4461,  ..., -2.3127, -0.0408,  0.3911],\n",
       "         [ 0.3505,  0.8841, -0.1659,  ..., -0.9632,  1.1264, -0.7043],\n",
       "         [ 0.5089,  0.8550, -1.1818,  ..., -0.5661,  1.7566, -0.2704]],\n",
       "\n",
       "        [[ 0.3528, -1.4555, -0.8705,  ..., -2.5673,  0.1229,  1.0867],\n",
       "         [-0.1983,  1.0654, -1.4940,  ..., -1.6413,  0.0457, -0.2922],\n",
       "         [ 0.4259, -0.3036, -2.2541,  ..., -1.2109, -0.7847,  0.7275],\n",
       "         ...,\n",
       "         [-0.8916,  1.3757, -1.1565,  ..., -2.4569,  0.5228,  0.7738],\n",
       "         [ 0.8238,  0.7375, -0.7992,  ..., -0.7496,  1.0537,  0.1120],\n",
       "         [ 1.0202,  0.4310,  0.2450,  ..., -1.6118,  0.2562, -0.6187]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0318, -1.1092, -0.0309,  ..., -0.5333,  1.0794, -0.9768],\n",
       "         [-0.8358,  0.8277, -2.2813,  ..., -0.3592,  1.1849,  1.5670],\n",
       "         [-1.1934, -1.4553,  0.1741,  ..., -2.1893,  1.2014,  0.6359],\n",
       "         ...,\n",
       "         [ 1.7666,  0.1585, -0.2111,  ..., -1.9580, -0.6025,  1.3213],\n",
       "         [ 0.1292, -1.4254, -0.4484,  ..., -0.9517,  0.9033, -0.9182],\n",
       "         [ 0.9094, -0.7348, -2.1168,  ..., -0.3305, -0.4613,  0.3582]],\n",
       "\n",
       "        [[ 0.4577, -0.2600,  0.1116,  ..., -1.7220,  1.5901, -0.9391],\n",
       "         [ 0.3778, -0.4791, -0.1648,  ..., -2.7550,  0.3161, -0.1812],\n",
       "         [ 0.2041,  1.0499, -0.4911,  ..., -0.7867, -0.0709,  0.9910],\n",
       "         ...,\n",
       "         [ 1.3683, -0.1094, -0.7718,  ..., -1.2137,  1.3598, -0.1015],\n",
       "         [ 0.0581, -0.7601, -1.3291,  ..., -1.6152,  0.4862,  0.9645],\n",
       "         [-1.5290, -0.5218, -1.4359,  ..., -1.1876,  0.3624, -0.4880]],\n",
       "\n",
       "        [[-0.2631, -0.6716,  0.0488,  ...,  0.5068,  0.1960,  0.4783],\n",
       "         [-0.0707, -0.8030, -1.9211,  ..., -0.9211, -0.5662,  0.1823],\n",
       "         [ 0.2915, -0.5627, -0.2045,  ..., -1.1015,  0.5505,  0.1179],\n",
       "         ...,\n",
       "         [ 0.6582,  0.7254, -1.0149,  ...,  0.0193,  1.1374, -0.6906],\n",
       "         [ 0.1465,  0.1739,  0.0843,  ..., -2.3035,  0.5739,  0.1570],\n",
       "         [ 0.8210,  1.1191, -0.3654,  ...,  0.8943,  1.7479, -0.2911]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand(128, 64, 384)\n",
    "tempM = Block(n_embed, n_head)\n",
    "tempM(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # This is the token embedding \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table=nn.Embedding(block_size, n_embed)\n",
    "        # decoderblocks (temp code for the future) \n",
    "        # We're using embeddings nn because sinosoidual is mainly for base transformer models\n",
    "        # We're building GPT, and that's what experts do. Transformers vs. GPT\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_F = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    def _init_weights(self, module):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, index, targets=None):\n",
    "\n",
    "        B, T = index.shape\n",
    "        # \n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        # print(f'Pre')\n",
    "        x = tok_emb + pos_emb\n",
    "        # print(f'tok + pos {x.shape}')\n",
    "        x = self.blocks(x)\n",
    "        # print(f'X selfblocks shape {x.shape}')\n",
    "        x = self.ln_F(x) # Layer Norm\n",
    "        logits = self.lm_head(x) # Linear Transformation\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None \n",
    "        else:\n",
    "            # B -> Batch\n",
    "            # T-> Seq Len\n",
    "            #C -> feature dimension\n",
    "            B,T,C = logits.shape\n",
    "            # Reshaping for the loss function\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # The logits and targets are flattened for the loss function which only\n",
    "            # Takes of these dimensions\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        return logits, loss\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        #index is (B,T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step (selects logits for previous batch)\n",
    "            logits = logits[:, -1, :] \n",
    "            # apply softmax for probs\n",
    "            probs=F.softmax(logits, dim=-1) # (B,C)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "m = model.to(device)\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "# generated_chars = decode(m.generate(context, max_new_tokens=500[0].toList()))\n",
    "#print(generated_chars)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "step: 0, train loss: 1.500, val loss: 1.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 100/10000 [05:15<4:33:40,  1.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 200/10000 [08:01<4:28:23,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 300/10000 [10:48<4:25:53,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 400/10000 [13:34<4:24:42,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 500/10000 [16:20<4:20:49,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "step: 500, train loss: 1.458, val loss: 1.460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 600/10000 [21:38<4:13:49,  1.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 700/10000 [24:29<4:13:23,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 800/10000 [27:14<4:12:16,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 900/10000 [30:02<4:09:05,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 1000/10000 [32:48<4:07:50,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "step: 1000, train loss: 1.503, val loss: 1.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 1100/10000 [38:07<4:05:53,  1.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 1200/10000 [40:52<4:00:14,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 1300/10000 [43:38<3:58:04,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 1400/10000 [46:24<3:55:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 1500/10000 [49:23<4:12:35,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "step: 1500, train loss: 1.476, val loss: 1.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 1600/10000 [55:08<4:17:41,  1.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 1700/10000 [58:18<4:38:25,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 1800/10000 [1:01:00<3:40:43,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 1900/10000 [1:03:41<3:36:16,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 2000/10000 [1:06:23<3:34:11,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "step: 2000, train loss: 1.436, val loss: 1.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 2100/10000 [1:11:37<3:32:24,  1.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 2200/10000 [1:14:19<3:29:54,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 2300/10000 [1:17:01<3:27:21,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 2400/10000 [1:19:42<3:23:27,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 2500/10000 [1:22:24<3:23:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "step: 2500, train loss: 1.466, val loss: 1.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 2600/10000 [1:27:36<3:17:32,  1.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 2700/10000 [1:30:16<3:14:54,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 2800/10000 [1:33:03<3:38:50,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 2900/10000 [1:35:53<3:20:16,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 3000/10000 [1:38:47<3:12:10,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "step: 3000, train loss: 1.441, val loss: 1.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 3100/10000 [1:44:03<3:06:27,  1.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 3200/10000 [1:46:46<3:03:07,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 3300/10000 [1:49:28<3:00:41,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 3400/10000 [1:52:10<2:57:49,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 3500/10000 [1:54:52<2:55:56,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "step: 3500, train loss: 1.425, val loss: 1.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▌      | 3600/10000 [2:00:04<2:52:43,  1.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 3700/10000 [2:02:46<2:50:13,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|███▊      | 3800/10000 [2:05:28<2:48:11,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 3900/10000 [2:08:13<3:24:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 4000/10000 [2:11:52<4:04:07,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "step: 4000, train loss: 1.420, val loss: 1.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  41%|████      | 4100/10000 [2:18:13<3:40:47,  2.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  42%|████▏     | 4200/10000 [2:21:36<3:12:28,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 4300/10000 [2:24:53<3:20:47,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|████▍     | 4400/10000 [2:27:49<2:24:39,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 4500/10000 [2:30:33<2:39:58,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "step: 4500, train loss: 1.437, val loss: 1.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  46%|████▌     | 4600/10000 [2:36:32<3:00:42,  2.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 4700/10000 [2:39:25<2:33:02,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████▊     | 4800/10000 [2:42:50<3:16:33,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▉     | 4900/10000 [2:45:55<2:21:20,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 5000/10000 [2:48:42<2:17:34,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "step: 5000, train loss: 1.400, val loss: 1.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|█████     | 5100/10000 [2:54:18<2:40:28,  1.97s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 5200/10000 [2:57:14<2:10:27,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|█████▎    | 5300/10000 [3:00:00<2:09:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  54%|█████▍    | 5399/10000 [3:02:45<2:35:44,  2.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\arjun\\OneDrive\\Desktop\\fcc-llm\\cuda\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\OneDrive\\Desktop\\fcc-llm\\cuda\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arjun\\OneDrive\\Desktop\\fcc-llm\\cuda\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "from tqdm import tqdm\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for iter in tqdm(range(max_iters), desc=\"Training Progress\"):\n",
    "    if iter % 100 ==0:\n",
    "        print(iter)\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f'step: {iter}, train loss: {losses[\"train\"]:.3f}, val loss: {losses[\"val\"]:.3f}')  # Fix quotes around dictionary keys\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model params\n",
      "loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model params\")\n",
    "with open('model-01.pkl', 'rb') as f:\n",
    "    model = pickle.load(f) \n",
    "print('loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(1.4215), 'val': tensor(1.4314)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calc Loss\n",
    "estimate_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion:\n",
      " This is how you make money, first step: I'm a head at a Company _and the upkerget arms who among there I had, behind and diving her with announces by the John Jiana and the\".[Lew Abiaclo da\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Prompt:\\n\")\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=150)[0].tolist())\n",
    "print(f'Completion:\\n {generated_chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Load & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
