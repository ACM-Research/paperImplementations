# Synopsis
*LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS* is yet another foundational ML paper which proposes a novel way of fine-tuning large, general purpose models on more specific downstream tasks. This paper draws from previous work from Meta, which showed that for many downstream tasks, many models had lower *intrinsic dimensions*. This means that only a small subset of the weights need to be modified for fine-tuning to be effective--the rest could be left frozen. This paper makes an additional jump and hypothesizes that the changes made to the weights are also of much lower intrinsic dimension, and can thus be represented as a product of two low rank matrices with little performance loss. With this configuration, all the original model weights can be frozen, and only the low-rank matrices affixed to whatever matrices you wish to adapt need be trained. This often means that only a fraction of a percent of weights need to be trained compared to full-model fine tuning for similar performance on the downstream task, saving training time, memory, and even inference latency. 

# Evaluation
This was a very impactful paper. Its biggest strength is the extension of the intrinsic dimension to the weight deltas, the idea on which the entire method proposed hinges on. Additionally, the jump from this to the fact that the adapter matrices could be represented as low-rank products is also insightful and necessary to get the impressive results they did. This paper has, so far, stood the test of time, and the only weakness I could pick out from it is that it is not very comprehensive. Though it states that the idea is model-agnostic in principle, the paper only uses language models in its experiments. They also limit themselves to adapting attention weight matrices, and explicitly "leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work." All in all, though, this is a very high quality and impactful paper.